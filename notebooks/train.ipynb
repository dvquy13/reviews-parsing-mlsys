{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqM0hhMuCfzn"
   },
   "source": [
    "# Reviews Parsing NER Aspects\n",
    "\n",
    "This notebook originates from [HuggingFace Token Classification Fine-tuning Tutorial](https://huggingface.co/learn/nlp-course/en/chapter7/2#defining-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(f\"../.env.{ENV}\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aHTw3y28Cfzq"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Documentation about the dataset: https://huggingface.co/datasets/dvquys/restaurant-reviews-public-sources\n",
    "raw_datasets = load_dataset(\"dvquys/restaurant-reviews-public-sources\", token=os.environ.get('HUGGINGFACE_READ_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bTLGN83_Cfzq",
    "outputId": "f65a8cb2-2020-4bdb-b19a-cf27d58584c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'Comments', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1590\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'text', 'Comments', 'tokens', 'ner_tags'],\n",
       "        num_rows: 398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'Comments', 'tokens', 'ner_tags'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jB3u92ewCfzq",
    "outputId": "6975e7d5-fc24-4f51-82f3-07c72aa1b8cd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'atmosphere',\n",
       " ',',\n",
       " 'combination',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'hottest',\n",
       " 'music',\n",
       " 'dress',\n",
       " 'code',\n",
       " 'is',\n",
       " 'relatively',\n",
       " 'strict',\n",
       " 'except',\n",
       " 'on',\n",
       " 'Fridays',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i5dJIifACfzq",
    "outputId": "ccb84f29-414f-459a-f12e-b75d6788c0c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, 14, 14, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tQ_vPx_zCfzq",
    "outputId": "e1fea2c6-3d40-4af7-a369-b5ac3ebd79e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-AMBIENCE', 'I-AMBIENCE', 'B-BEVERAGE', 'I-BEVERAGE', 'B-FOOD', 'I-FOOD', 'B-LOCATION', 'I-LOCATION', 'B-OVERALL', 'I-OVERALL', 'B-PRICE', 'I-PRICE', 'B-SERVICE', 'I-SERVICE', 'B-STAFF', 'I-STAFF', 'B-VALUE', 'I-VALUE', 'B-VIEW', 'I-VIEW'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KAIpiIwkCfzq",
    "outputId": "762dcbdc-1346-43ff-bb17-b9b3e5fe84da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-AMBIENCE',\n",
       " 'I-AMBIENCE',\n",
       " 'B-BEVERAGE',\n",
       " 'I-BEVERAGE',\n",
       " 'B-FOOD',\n",
       " 'I-FOOD',\n",
       " 'B-LOCATION',\n",
       " 'I-LOCATION',\n",
       " 'B-OVERALL',\n",
       " 'I-OVERALL',\n",
       " 'B-PRICE',\n",
       " 'I-PRICE',\n",
       " 'B-SERVICE',\n",
       " 'I-SERVICE',\n",
       " 'B-STAFF',\n",
       " 'I-STAFF',\n",
       " 'B-VALUE',\n",
       " 'I-VALUE',\n",
       " 'B-VIEW',\n",
       " 'I-VIEW']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0xeTcqAMCfzr",
    "outputId": "6ab430b4-8a33-4ec8-c330-dbe9b52fca28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lobster sandwich is     good   and the spaghetti with   Scallops and    Shrimp is     great  . \n",
      "O   B-FOOD  I-FOOD   I-FOOD I-FOOD O   O   B-FOOD    I-FOOD I-FOOD   I-FOOD I-FOOD I-FOOD I-FOOD O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][1][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][1][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jNTJWPN0Cfzr"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace the model_checkpoint with any other model we prefer from the Hub, or with a local folder in which you‚Äôve saved a pretrained model and a tokenizer. The only constraint is that the tokenizer needs to be backed by the ü§ó Tokenizers library, so there‚Äôs a ‚Äúfast‚Äù version available. You can see all the architectures that come with a fast version in this big table, and to check that the tokenizer object you‚Äôre using is indeed backed by ü§ó Tokenizers you can look at its is_fast attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "95af615CCfzr",
    "outputId": "4bb43d70-35a0-4839-c762-7ce7645b4238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize a pre-tokenized input, we can use our tokenizer as usual and just add is_split_into_words=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CYeL5AGvCfzr",
    "outputId": "228eccc8-d5c4-41fb-84c4-e82d595e3d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'The',\n",
       " 'lo',\n",
       " '##bs',\n",
       " '##ter',\n",
       " 'sandwich',\n",
       " 'is',\n",
       " 'good',\n",
       " 'and',\n",
       " 'the',\n",
       " 'spa',\n",
       " '##gh',\n",
       " '##etti',\n",
       " 'with',\n",
       " 'Sc',\n",
       " '##allo',\n",
       " '##ps',\n",
       " 'and',\n",
       " 'Shri',\n",
       " '##mp',\n",
       " 'is',\n",
       " 'great',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_idx = 1\n",
    "inputs = tokenizer(raw_datasets[\"train\"][example_idx][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the tokenizer added the special tokens used by the model ([CLS] at the beginning and [SEP] at the end) and left most of the words untouched. Some works like \"lobster\", however, would be tokenized into multiple subwords, \"lo\", \"##bs\" and \"##ter\". This introduces a mismatch between our inputs and the labels. Accounting for the special tokens is easy (we know they are at the beginning and the end), but we also need to make sure we align all the labels with the proper words.\n",
    "\n",
    "Fortunately, because we‚Äôre using a fast tokenizer we have access to the ü§ó Tokenizers superpowers, which means we can easily map each token to its corresponding word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Vi7JvpDuCfzr",
    "outputId": "a4a3dd52-90e4-4f59-8bd5-38342d75c8a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a tiny bit of work, we can then expand our label list to match the tokens. The first rule we‚Äôll apply is that special tokens get a label of -100. This is because by default -100 is an index that is ignored in the loss function we will use (cross entropy). Then, each token gets the same label as the token that started the word it‚Äôs inside, since they are part of the same entity. For tokens inside a word but not at the beginning, we replace the B- with I- (since the token does not begin the entity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8daUmNWECfzr"
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sJSVH2yPCfzr",
    "outputId": "79074771-98a9-4f45-d458-fa0e3445b8af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 6, 6, 6, 0, 0, 5, 6, 6, 6, 6, 6, 6, 0]\n",
      "[-100, 0, 5, 6, 6, 6, 6, 6, 0, 0, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][example_idx][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our function added the -100 for the two special tokens at the beginning and the end, and a new 0 for our word that was split into two tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess our whole dataset, we need to tokenize all the inputs and apply align_labels_with_tokens() on all the labels. To take advantage of the speed of our fast tokenizer, it‚Äôs best to tokenize lots of texts at the same time, so we‚Äôll write a function that processes a list of examples and use the Dataset.map() method with the option batched=True. The only thing that is different from our previous example is that the word_ids() function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dG9xlu1nCfzr"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply all that preprocessing in one go on the other splits of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "suLAZY02Cfzr"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning with custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizers warnings when constructing pipelines\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been tokenized but to be able to input them into our model, we also need to pad them.\n",
    "\n",
    "Here our labels should be padded the exact same way as the inputs so that they stay the same size, using -100 as a value so that the corresponding predictions are ignored in the loss computation.\n",
    "\n",
    "This is all done by a DataCollatorForTokenClassification. Like the DataCollatorWithPadding, it takes the tokenizer used to preprocess the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qk_H33PDCfzr"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this on a few samples, we can just call it on a list of examples from our tokenized training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ducHQw9dCfzr",
    "outputId": "b0619874-b40e-4933-ffb3-d6483244ad90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   13,\n",
       "           14,   14,   14,   14,    0,    0,    0,    0,    0, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    5,    6,    6,    6,    6,    6,    0,    0,    5,    6,\n",
       "            6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    0, -100,\n",
       "         -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,   13,   14,   14,\n",
       "           14,   14,    0,    0,    0,    0,    0,   19,   20,   20,   20,    0,\n",
       "         -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    1,    2,    2,    2,    2,    2,    0,    0,    0,   13,\n",
       "           14,   14,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0, -100],\n",
       "        [-100,    0,    3,    4,    4,    4,    4,    4,    0,    1,    2,    2,\n",
       "            2,    2,    2,    2,    0, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs compare this to the labels for the first and second elements in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "q1Ev204qCfzr",
    "outputId": "f9591b5d-255e-41f4-de6a-29b33888749e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, 14, 14, 0, 0, 0, 0, 0, -100]\n",
      "[-100, 0, 5, 6, 6, 6, 6, 6, 0, 0, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, 14, 14, 0, 0, 0, 0, 0, 19, 20, 20, 20, 0, -100]\n",
      "[-100, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "[-100, 0, 3, 4, 4, 4, 4, 4, 0, 1, 2, 2, 2, 2, 2, 2, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes might not be clearly visible but we should observe that some shorter inputs would have their padded formats added with -100 tokens to have the same length as others.\n",
    "\n",
    "Let's apply the padding as the `collate_fn` used in DataLoader API to prepare our datasets for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"val\"], collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on a token classification problem, we will use the AutoModelForTokenClassification class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the num_labels argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it‚Äôs better to set the correct label correspondences instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-SERVICE',\n",
       " 'I-SERVICE',\n",
       " 'I-SERVICE',\n",
       " 'I-SERVICE',\n",
       " 'I-SERVICE',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping between Label int ID and its name should be set by two dictionaries, id2label and label2id, which contain the mappings from ID to label and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-AMBIENCE',\n",
       " 2: 'I-AMBIENCE',\n",
       " 3: 'B-BEVERAGE',\n",
       " 4: 'I-BEVERAGE',\n",
       " 5: 'B-FOOD',\n",
       " 6: 'I-FOOD',\n",
       " 7: 'B-LOCATION',\n",
       " 8: 'I-LOCATION',\n",
       " 9: 'B-OVERALL',\n",
       " 10: 'I-OVERALL',\n",
       " 11: 'B-PRICE',\n",
       " 12: 'I-PRICE',\n",
       " 13: 'B-SERVICE',\n",
       " 14: 'I-SERVICE',\n",
       " 15: 'B-STAFF',\n",
       " 16: 'I-STAFF',\n",
       " 17: 'B-VALUE',\n",
       " 18: 'I-VALUE',\n",
       " 19: 'B-VIEW',\n",
       " 20: 'I-VIEW'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just pass them to the AutoModelForTokenClassification.from_pretrained() method, and they will be set in the model‚Äôs configuration and then properly saved and uploaded to the Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have sent our train_dataloader to accelerator.prepare(), we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "# Try num_train_epochs = 10 for the first model attempt, found that model accuracy does not improve after 5 epochs\n",
    "# Do not set to 5 because at 5 its output is not as good (eye-balling)\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the HuggingFace model repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tgrC6azuCfzu",
    "outputId": "3ec9f22e-0269-4de8-a40d-c46b67a31565"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dvquys/ner-finetune-restaurant-reviews-aspects'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"ner-finetune-restaurant-reviews-aspects\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dvquys/ner-finetune-restaurant-reviews-aspects/commit/340b0ec3b9226cae8dca3fde9bb418c1a81677a0', commit_message='Upload README.md with huggingface_hub', commit_description='', oid='340b0ec3b9226cae8dca3fde9bb418c1a81677a0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import whoami, create_repo, ModelCardData, ModelCard\n",
    "\n",
    "repo_id = repo_name\n",
    "url = create_repo(repo_id, exist_ok=True, token=os.environ.get(\"HUGGINGFACE_WRITE_TOKEN\"))\n",
    "\n",
    "card_data = ModelCardData(\n",
    "    language='en',\n",
    "    datasets='dvquys/restaurant-reviews-public-sources',\n",
    "    license='mit',\n",
    "    library_name='pytorch',\n",
    "    tags=['ner', 'reviews', 'fine-tune', 'token classification']\n",
    ")\n",
    "model_description = \"\"\"\n",
    "# Reviews Parsing NER Aspects\n",
    "\n",
    "This model takes a text review as input and output the parsed aspects mentioned which spans over the entity and the sentiment text.\n",
    "\n",
    "It's based on the idea of fine-tuning a base LLM with a token classification task.\n",
    "\n",
    "More info: https://huggingface.co/learn/nlp-course/en/chapter7/2#token-classification\n",
    "\"\"\"\n",
    "card = ModelCard.from_template(\n",
    "    card_data,\n",
    "    model_id=model_name,\n",
    "    model_description=model_description,\n",
    "    developers=\"Quy Dinh\",\n",
    "    repo=\"https://github.com/huggingface/huggingface_hub\",\n",
    ")\n",
    "\n",
    "card.push_to_hub(repo_id, token=os.environ.get(\"HUGGINGFACE_WRITE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Repository instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AUH5cHetCfzu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/reviews-parsing-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/home/dvquys/frostmourne/reviews-parsing-mlsys/notebooks/ner-finetune-restaurant-reviews-aspects is already a clone of https://huggingface.co/dvquys/ner-finetune-restaurant-reviews-aspects. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "output_dir = model_name\n",
    "repo = Repository(output_dir, clone_from=repo_name, token=os.environ.get(\"HUGGINGFACE_WRITE_TOKEN\"))\n",
    "repo.git_pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_on_evalset(model, evalset, metric):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        model: Transformers model\n",
    "        evalset: HuggingFace dataset (train, eval, test) in Data Loader format\n",
    "        metric: a metric instance initiated by `import evaluate; metric = evaluate.load(\"seqeval\")`\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    for batch in evalset:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch.to(device))\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "\n",
    "    return results\n",
    "\n",
    "def log_evaluation_metrics(results, prefix='eval', to_mlflow=True, step=None):\n",
    "    results_reformatted = {}\n",
    "    aggregated = dict()\n",
    "    for key, value in results.items():\n",
    "        if key.startswith('overall_'):\n",
    "            assert isinstance(value, float)\n",
    "            metric = key.replace('overall_', '')\n",
    "            metric_key = f\"{prefix}_aggregated_{metric}\"\n",
    "            aggregated[metric] = value\n",
    "            if to_mlflow:\n",
    "                mlflow.log_metric(metric_key, value, step=step)\n",
    "        else:\n",
    "            label = key\n",
    "            for metric, metric_value in value.items():\n",
    "                metric_key = f\"{prefix}_{key}_{metric}\"\n",
    "                if to_mlflow:\n",
    "                    mlflow.log_metric(metric_key, metric_value, step=step)\n",
    "            results_reformatted.update({key: value})\n",
    "    results_reformatted.update({\"aggregated\": aggregated})\n",
    "    results_reformatted_df = pd.DataFrame.from_dict(results_reformatted, orient='index')\n",
    "    logger.info(f\"\\n{results_reformatted_df}\")\n",
    "    return results_reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2107e530a72b4b6db1d45409b202f3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fde2441020e42b9b25d76a63a8ebd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/reviews-parsing-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-08-04 15:39:09.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mevaluation on val set at epoch 0:\u001b[0m\n",
      "\u001b[32m2024-08-04 15:39:36.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_evaluation_metrics\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1m\n",
      "            precision    recall        f1  number  accuracy\n",
      "AMBIENCE     0.226804  0.213592  0.220000   103.0       NaN\n",
      "BEVERAGE     0.025641  0.142857  0.043478     7.0       NaN\n",
      "FOOD         0.416370  0.258278  0.318801   453.0       NaN\n",
      "LOCATION     0.000000  0.000000  0.000000     0.0       NaN\n",
      "OVERALL      0.000000  0.000000  0.000000     0.0       NaN\n",
      "PRICE        0.000000  0.000000  0.000000     0.0       NaN\n",
      "SERVICE      0.283422  0.170418  0.212851   311.0       NaN\n",
      "STAFF        0.000000  0.000000  0.000000     0.0       NaN\n",
      "VALUE        0.000000  0.000000  0.000000     0.0       NaN\n",
      "VIEW         0.000000  0.000000  0.000000     0.0       NaN\n",
      "aggregated   0.298300  0.220824  0.253780     NaN  0.696058\u001b[0m\n",
      "\u001b[32m2024-08-04 15:39:37.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mPushing to HuggingFace Hub...\u001b[0m\n",
      "/home/dvquys/frostmourne/reviews-parsing-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-08-04 15:40:06.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mevaluation on val set at epoch 1:\u001b[0m\n",
      "\u001b[32m2024-08-04 15:40:31.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_evaluation_metrics\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1m\n",
      "            precision    recall        f1  number  accuracy\n",
      "AMBIENCE     0.350515  0.237762  0.283333   143.0       NaN\n",
      "BEVERAGE     0.333333  0.194030  0.245283    67.0       NaN\n",
      "FOOD         0.430605  0.281395  0.340366   430.0       NaN\n",
      "LOCATION     0.000000  0.000000  0.000000     0.0       NaN\n",
      "OVERALL      0.000000  0.000000  0.000000     0.0       NaN\n",
      "PRICE        0.000000  0.000000  0.000000    20.0       NaN\n",
      "SERVICE      0.336898  0.229091  0.272727   275.0       NaN\n",
      "STAFF        0.000000  0.000000  0.000000     0.0       NaN\n",
      "VALUE        0.000000  0.000000  0.000000     2.0       NaN\n",
      "VIEW         0.000000  0.000000  0.000000     0.0       NaN\n",
      "aggregated   0.357032  0.246531  0.291667     NaN   0.72247\u001b[0m\n",
      "\u001b[32m2024-08-04 15:40:32.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mPushing to HuggingFace Hub...\u001b[0m\n",
      "Several commits (2) will be pushed upstream.\n",
      "/home/dvquys/frostmourne/reviews-parsing-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-08-04 15:41:02.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mevaluation on val set at epoch 2:\u001b[0m\n",
      "\u001b[32m2024-08-04 15:41:26.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_evaluation_metrics\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1m\n",
      "            precision    recall        f1  number  accuracy\n",
      "AMBIENCE     0.340206  0.270492  0.301370   122.0       NaN\n",
      "BEVERAGE     0.358974  0.222222  0.274510    63.0       NaN\n",
      "FOOD         0.448399  0.293023  0.354430   430.0       NaN\n",
      "LOCATION     0.000000  0.000000  0.000000     0.0       NaN\n",
      "OVERALL      0.000000  0.000000  0.000000     0.0       NaN\n",
      "PRICE        0.000000  0.000000  0.000000    23.0       NaN\n",
      "SERVICE      0.331551  0.213058  0.259414   291.0       NaN\n",
      "STAFF        0.000000  0.000000  0.000000     0.0       NaN\n",
      "VALUE        0.000000  0.000000  0.000000     6.0       NaN\n",
      "VIEW         0.000000  0.000000  0.000000     0.0       NaN\n",
      "aggregated   0.363215  0.251337  0.297092     NaN  0.722996\u001b[0m\n",
      "\u001b[32m2024-08-04 15:41:27.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mPushing to HuggingFace Hub...\u001b[0m\n",
      "Several commits (3) will be pushed upstream.\n",
      "/home/dvquys/frostmourne/reviews-parsing-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[32m2024-08-04 15:41:36.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mevaluation on test set after training:\u001b[0m\n",
      "\u001b[32m2024-08-04 15:41:49.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlog_evaluation_metrics\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1m\n",
      "            precision    recall        f1  number  accuracy\n",
      "AMBIENCE     1.000000  0.333333  0.500000     3.0       NaN\n",
      "FOOD         0.125000  0.058824  0.080000    17.0       NaN\n",
      "LOCATION     0.000000  0.000000  0.000000     0.0       NaN\n",
      "OVERALL      0.000000  0.000000  0.000000     0.0       NaN\n",
      "SERVICE      0.250000  0.200000  0.222222     5.0       NaN\n",
      "VIEW         0.000000  0.000000  0.000000     0.0       NaN\n",
      "aggregated   0.176471  0.120000  0.142857     NaN  0.506073\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4193efcd86474dd19ea38b61eac7d0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-04 15:42:31.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1msignature=inputs: \n",
      "  [string (required)]\n",
      "outputs: \n",
      "  [Array({end: long (required), entity_group: string (required), score: float (required), start: long (required), word: string (required)}) (required)]\n",
      "params: \n",
      "  None\n",
      "\u001b[0m\n",
      "2024/08/04 15:42:32 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository dvquys/ner-finetune-restaurant-reviews-aspects will be logged instead.\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "task_name = \"token-classification\"\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "mlflow.set_experiment(\"Reviews Parsing NER Aspects - OSS LLM training data\")\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"num_train_epochs\", num_train_epochs)\n",
    "    mlflow.log_param(\"num_update_steps_per_epoch\", num_update_steps_per_epoch)\n",
    "    mlflow.log_param(\"num_training_steps\", num_training_steps)\n",
    "    mlflow.log_param(\"learning_rate\", optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    \n",
    "    for epoch in range(num_train_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "    \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "        mlflow.log_metric(\"train_loss\", loss.item(), step=epoch)\n",
    "    \n",
    "        # Evaluation\n",
    "        results = evaluate_on_evalset(model, val_dataloader, metric)\n",
    "        logger.info(f\"evaluation on val set at epoch {epoch}:\")\n",
    "        log_evaluation_metrics(results, prefix='eval', step=epoch)\n",
    "    \n",
    "        # Save and upload\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            logger.info(f\"Pushing to HuggingFace Hub...\")\n",
    "            repo.push_to_hub(\n",
    "                commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "            )\n",
    "\n",
    "    results = evaluate_on_evalset(model, test_dataloader, metric)\n",
    "    logger.info(f\"evaluation on test set after training:\")\n",
    "    log_evaluation_metrics(results, prefix='test')\n",
    "\n",
    "    # Log model to MLflow\n",
    "    # Should use model=repo_name here instead of output_dir\n",
    "    ner_aspect_pipeline = pipeline(\n",
    "        task_name, model=repo_name, aggregation_strategy=\"simple\", device='cuda'\n",
    "    )\n",
    "    input_examples = raw_datasets[\"train\"][:5]['text']\n",
    "    signature = mlflow.models.infer_signature(input_examples, ner_aspect_pipeline(input_examples))\n",
    "    logger.info(f\"{signature=}\")\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        task=task_name,\n",
    "        transformers_model=ner_aspect_pipeline,\n",
    "        artifact_path=\"ner_aspect\",\n",
    "        input_example=input_examples,\n",
    "        # Set example_no_conversion=True based on this issue: https://github.com/mlflow/mlflow/issues/12384\n",
    "        example_no_conversion=True,\n",
    "        signature=signature,\n",
    "        # Uncomment the following line to save the model in 'reference-only' mode:\n",
    "        save_pretrained=False,\n",
    "    )\n",
    "    readme = \"\"\"\n",
    "The model should be loaded with the `transformers` flavor since it returns more usable output format compared to the `pyfunc` flavor.\n",
    "\n",
    "Example:\n",
    "```\n",
    "token_classifier_mlflow_pipeline = mlflow.transformers.load_model(model_uri=model_info.model_uri, return_type=\"pipeline\", aggregation_strategy=\"simple\")\n",
    "token_classifier_mlflow_pipeline(['Delicious food friendly staff and one good celebration!', 'What an amazing dining experience'])\n",
    "```\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        path = Path(tmp_dir, \"README.md\")\n",
    "        path.write_text(readme)\n",
    "        mlflow.log_artifact(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Via Transformers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f13d5399ae4b1597504debfb947233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local model\n",
    "token_classifier = pipeline(\n",
    "    task_name, model=repo_name, aggregation_strategy=\"simple\", device='cuda'\n",
    ")\n",
    "token_classifier('Delicious food friendly staff and one good celebration!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Via MLflow Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use transformers.load_model() to get the correct behavior or the Transformers pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-04 15:43:24.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mLoading model from MLflow Registry at model_info.model_uri='runs:/474c54ec1a8943fd986aa48e9c33f636/ner_aspect'...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading model from MLflow Registry at {model_info.model_uri=}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9f3ccd293845909c0b2a4db60c59b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/04 15:43:31 INFO mlflow.transformers: 'runs:/474c54ec1a8943fd986aa48e9c33f636/ner_aspect' resolved as 'mlflow-artifacts:/1/474c54ec1a8943fd986aa48e9c33f636/artifacts/ner_aspect'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28d1af455b24565bf27d6716a200257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/04 15:43:32 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    }
   ],
   "source": [
    "token_classifier_mlflow_pipeline = mlflow.transformers.load_model(model_uri=model_info.model_uri, return_type=\"pipeline\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [{'entity_group': 'FOOD',\n",
       "   'score': 0.46381864,\n",
       "   'word': 'dining',\n",
       "   'start': 16,\n",
       "   'end': 22}]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = token_classifier_mlflow_pipeline(['Delicious food friendly staff and one good celebration!', 'What an amazing dining experience'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[[], [{\"entity_group\": \"FOOD\", \"score\": 0.46381863951683044, \"word\": \"dining\", \"start\": 16, \"end\": 22}]]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "for predictions in output:\n",
    "    for prediction in predictions:\n",
    "        prediction['score'] = float(prediction['score'])\n",
    "json.dumps(output).encode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyFunc would output output in an unexpected and unusable way"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "token_classifier_mlflow = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n",
    "token_classifier_mlflow.predict(['Delicious food friendly staff and one good celebration!', 'What an amazing dining experience'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mlflow connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-04 19:22:46.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mmlflow.get_tracking_uri()='https://dev-reviews-parsing-mlsys.endpoints.cold-embrace-240710.cloud.goog/mlflow'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"{mlflow.get_tracking_uri()=}\")\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    with open(\"example_artifact.txt\", \"w\") as f:\n",
    "        f.write(\"This is an example artifact.\")\n",
    "    mlflow.log_artifact(\"example_artifact.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token classification (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
